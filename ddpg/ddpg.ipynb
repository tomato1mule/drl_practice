{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "\n",
    "import gym\n",
    "\n",
    "from utils import ReplayBuffer,clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, N_s, N_a, max_a):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(N_s,200)\n",
    "        self.layer_2 = nn.Linear(200,200)\n",
    "        self.layer_3 = nn.Linear(200,N_a)\n",
    "        self.N_s = N_s\n",
    "        self.N_a = N_a\n",
    "        self.max_a = torch.tensor(max_a, dtype = torch.float32).to(device)\n",
    "    \n",
    "    def forward(self, s):\n",
    "        assert type(s) == torch.Tensor\n",
    "        \n",
    "        h = F.leaky_relu(self.layer_1(s))\n",
    "        h = F.leaky_relu(self.layer_2(h))\n",
    "        h = torch.tanh(self.layer_3(h)) * self. max_a\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self,N_s,N_a):\n",
    "        super().__init__()\n",
    "        self.N_s = N_s\n",
    "        self.N_a = N_a\n",
    "        \n",
    "        self.layer1 = nn.Linear(N_s+N_a,200)\n",
    "        self.layer2 = nn.Linear(200,200)\n",
    "        self.layer3 = nn.Linear(200,1)\n",
    "    \n",
    "    def forward(self,s,a):\n",
    "        x = torch.cat((s,a),dim=-1)\n",
    "        \n",
    "        Q = F.leaky_relu(self.layer1(x))\n",
    "        Q = F.leaky_relu(self.layer2(Q))\n",
    "        Q = self.layer3(Q)\n",
    "        \n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    def __init__(self, N_s, N_a, max_a):\n",
    "        self.N_s = N_s\n",
    "        self.N_a = N_a\n",
    "        self.max_a = max_a\n",
    "        \n",
    "        self.actor = Actor(N_s, N_a, max_a)\n",
    "        self.critic = Critic(N_s, N_a)\n",
    "        self.target_critic = Critic(N_s, N_a)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "    def to(self, device):\n",
    "        self.actor.to(device)\n",
    "        self.critic.to(device)\n",
    "        self.target_critic.to(device)\n",
    "        \n",
    "    def polyak(self, tau):\n",
    "        for param, target_param in zip(self.critic.parameters(),self.target_critic.parameters()):\n",
    "            target_param.data.copy_(tau*param.data + (1-tau)*target_param.data)\n",
    "    \n",
    "    def target_Q(self, batch, gamma):\n",
    "        NS = torch.tensor(batch['next_state'], dtype=torch.float32).to(device)\n",
    "        R = torch.tensor(batch['reward'], dtype=torch.float32).to(device)\n",
    "        D = torch.tensor(batch['done'],dtype=torch.int8).to(device)\n",
    "        R = R.view(*R.shape,1)\n",
    "        D = D.view(*D.shape,1)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            A = self.actor(NS)\n",
    "            NQ = self.target_critic(NS,A)\n",
    "            \n",
    "            return R + ( gamma * (~D) * NQ )\n",
    "    \n",
    "    def critic_loss(self, batch, gamma):\n",
    "        S = torch.tensor(batch['state'], dtype=torch.float32).to(device)\n",
    "        A = torch.tensor(batch['action'],dtype=torch.float32).to(device)\n",
    "        \n",
    "        Q = self.critic(S,A)\n",
    "        target_Q = self.target_Q(batch, gamma)\n",
    "        \n",
    "        return F.mse_loss(Q,target_Q)\n",
    "    \n",
    "    def actor_loss(self, batch):\n",
    "        S = torch.tensor(batch['state'], dtype=torch.float32).to(device)\n",
    "        A = self.actor(S)\n",
    "        loss = -self.critic(S,A).mean()\n",
    "        #loss = -self.target_critic(S,A).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_critic_once(self, batch, gamma=0.99, tau=0.005):\n",
    "        self.critic_optim.zero_grad()\n",
    "        L = self.critic_loss(batch, gamma)\n",
    "        L.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        self.polyak(tau)\n",
    "        \n",
    "        return L.item()\n",
    "    \n",
    "    def train_actor_once(self, batch):\n",
    "        self.actor_optim.zero_grad()\n",
    "        L = self.actor_loss(batch)\n",
    "        L.backward()\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        return L.item()\n",
    "    \n",
    "    def explore(self,s,sigma = 0.3):\n",
    "        s = torch.tensor(s,dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            a = self.actor(s)\n",
    "        a = a.detach().cpu().numpy()\n",
    "        eps = np.random.normal(size = self.N_a, scale = sigma)\n",
    "        \n",
    "        return clip(a + eps,-self.max_a,self.max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, buffer, model, render = False, sigma = 0.3):\n",
    "    s = env.reset()\n",
    "    done = 0\n",
    "    if render:\n",
    "        env.render()\n",
    "        \n",
    "    R = 0\n",
    "    while(not done):\n",
    "        a = model.explore(s,sigma)\n",
    "        ns, r, done, info = env.step(a)\n",
    "        T = [s,ns,a,r,done]\n",
    "        buffer.append(T)\n",
    "        s = ns.copy()\n",
    "        if render:\n",
    "            env.render()\n",
    "        R += r\n",
    "        \n",
    "    return R\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(max_len = 1e6)\n",
    "env = gym.make('HalfCheetah-v2')\n",
    "\n",
    "model = DDPG(17,6, np.array([1., 1., 1., 1., 1., 1., ]))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add initial episodes to buffer w/o training\n",
    "for episode in tqdm(range(3)):\n",
    "    R = run_episode(env, buffer, model, render = False, sigma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:01<00:00,  1.66it/s,  =R: -163.850, sigma: 0.040]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "N_episodes = 1000\n",
    "pbar = tqdm(range(N_episodes))\n",
    "for episode in pbar:\n",
    "    render = False\n",
    "    sigma = 0.8 * np.exp(-3*episode/N_episodes)\n",
    "        \n",
    "    R = run_episode(env, buffer, model, render = render, sigma = sigma)\n",
    "    pbar.set_postfix({' ' : f'R: {R:.3f}, sigma: {sigma:.3f}'})\n",
    "    \n",
    "    \n",
    "    \n",
    "    for n in range(20):\n",
    "        batch = buffer.sample(64)\n",
    "        critic_L = model.train_critic_once(batch)\n",
    "        if n%2 == 0:\n",
    "            actor_L = model.train_actor_once(batch)\n",
    "            #print(f\"critic loss: {critic_L}\")\n",
    "            #print(f\"actor loss: {actor_L}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for episode in range(1):\n",
    "    run_episode(env, buffer, model, render = True, sigma = 0.0)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
